import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import zipfile
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

# Función para procesar un archivo descargado
def process_file(link, folder_name):
    # Descargar el archivo desde el enlace
    zip_file_name = link.split('/')[-1].split("=")[-1]
    zip_file_path = os.path.join(folder_name, zip_file_name)
    response = requests.get(link)
    if response.status_code == 200:
        with open(zip_file_path, 'wb') as file:
            file.write(response.content)
    # Extraer los archivos del archivo ZIP
    extracted_folder = os.path.splitext(zip_file_path)[0]
    os.makedirs(extracted_folder, exist_ok=True)
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extracted_folder)
    # Procesar los archivos extraídos
    extracted_files = os.listdir(extracted_folder)
    for file_name in extracted_files:
        new_file_name = os.path.splitext(file_name)[0] + ".csv"
        original_file_path = os.path.join(extracted_folder, file_name)
        new_file_path = os.path.join(extracted_folder, new_file_name)
        df = pd.read_csv(original_file_path, delimiter=';', encoding='latin-1', header=None, dtype=str, low_memory=False)
        df.columns = df.iloc[1]
        df = df.drop([0,1]).reset_index(drop=True)
        df = df.rename(columns={'Energía Compra/Venta': 'Energia Compra/Venta'})
        df.to_csv(new_file_path, index=False)
        os.remove(original_file_path)
    os.remove(zip_file_path)
    print(f"Los documentos del archivo {zip_file_name} ya se han descargado y procesado")

# URL de la página web
url = 'https://www.omie.es/en/file-access-list?parents%5B0%5D=/&parents%5B1%5D=Day-ahead%20Market&parents%5B2%5D=3.%20Curves&dir=Monthly%20files%20with%20aggregate%20supply%20and%20demand%20curves%20of%20Day-ahead%20market%20including%20bid%20units&realdir=curva_pbc_uof'
response = requests.get(url)
html = response.text
soup = BeautifulSoup(html, 'lxml')

# Encontrar todos los enlaces que contienen "file-download?parents%5B0%5D"
all_links = soup.find_all('a', href=lambda href: href and "file-download?parents%5B0%5D" in href)
# Añadir «https://www.omie.es» a los enlaces y almacenarlos en una lista
download_links = ["https://www.omie.es" + link.get('href') for link in all_links]

# Crear una carpeta para guardar los archivos descargados en el escritorio
desktop_path = str(Path.home() / "Desktop")
folder_name = os.path.join(desktop_path, "downloaded_files")
os.makedirs(folder_name, exist_ok=True)

# Utilizar ThreadPoolExecutor para descargar y procesar los archivos en paralelo
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = []
    for link in download_links[0:5]:
        futures.append(executor.submit(process_file, link, folder_name))
    for future in futures:
        future.result()
        
        
        
#PREGUNTA 2: Limpiar y preparar los datos (eliminar “outliers”, gestionar “missings”,..)

#empezamos abriendo los ficheros csv y insertando su informacion en un data frame (dfs)

import os
from pathlib import Path

# Ruta a la carpeta que contiene las subcarpetas con los archivos CSV
base_folder = Path.home() / "Desktop" / "downloaded_files"

# Inicializar una lista para almacenar los DataFrames
dfs = []

# Recorrer las carpetas que coinciden con el patrón "curva_pbc_uof_0XXYY"
for folder in base_folder.iterdir():
    if folder.is_dir() and folder.name.startswith("curva_pbc_uof_"):
        for csv_file in folder.glob("*.csv"):
            # Leer el archivo CSV
            df = pd.read_csv(csv_file, delimiter=',', encoding='latin-1', dtype=str, low_memory=False)
            # Añadir el DataFrame a la lista
            dfs.append(df)

# Combinar todos los DataFrames en uno solo
combined_df = pd.concat(dfs, ignore_index=True)
#Quitamos la ultima columna que esta vacia
combined_df = combined_df.iloc[:, :-1]

# Mostrar una muestra de los datos combinados
print(combined_df.head())

#Ahora ya podemos empezar a limpiar los datos mas en detalle
# Eliminar filas duplicadas si las hay
combined_df.drop_duplicates(inplace=True)

# Limpiar los valores en la columna 'Precio Compra/Venta' para eliminar la coma y convertirlos a valores flotantes--> por ejemplo que 1.500,1 sea 1500.1
combined_df['Precio Compra/Venta'] = combined_df['Precio Compra/Venta'].str.replace('.', '').str.replace(',', '.').astype(float)

# Convertir columnas relevantes a tipos de datos apropiados
combined_df['Fecha'] = pd.to_datetime(combined_df['Fecha'])
combined_df['Precio Compra/Venta'] = combined_df['Precio Compra/Venta'].astype(float)

# Gestionar valores perdidos
# Supongamos que queremos llenar los valores faltantes en la columna 'Precio Compra/Venta' con la mediana de esa columna
combined_df['Precio Compra/Venta'].fillna(combined_df['Precio Compra/Venta'].median(), inplace=True)

# Manejar outliers
# Supongamos que queremos eliminar los valores atípicos en la columna 'Precio Compra/Venta' utilizando el rango intercuartílico (IQR)
Q1 = combined_df['Precio Compra/Venta'].quantile(0.25)
Q3 = combined_df['Precio Compra/Venta'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
combined_df = combined_df[(combined_df['Precio Compra/Venta'] >= lower_bound) & (combined_df['Precio Compra/Venta'] <= upper_bound)]

# Resetear el índice después de eliminar filas
combined_df.reset_index(drop=True, inplace=True)

# Mostrar una muestra de los datos limpios y preparados
print(combined_df.head())

#Ver los types de los datos
print(combined_df.dtypes)
#Hacemos str al float
combined_df['Precio Compra/Venta']=combined_df['Precio Compra/Venta'].astype(str)
combined_df['Fecha']=combined_df['Fecha'].astype(str)
print(combined_df.dtypes)


#PREGUNTA 3: Visualizacio n de los datos y extraccio n de “insights” u tiles para explicar la información oculta
#dentro de los datos, mediante el uso de gra ficos de matplotlib o plotly.

import matplotlib.pyplot as plt

plt.plot(combined_df['Hora'],combined_df['Precio Compra/Venta'])
plt.title('Precios por hora')
plt.xlabel('Hora')
plt.ylabel('Precio Compra/Venta')
plt.tight_layout()
plt.show()
